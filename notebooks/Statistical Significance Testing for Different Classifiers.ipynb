{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d08aa3c2",
   "metadata": {},
   "source": [
    "# Statistical Significance Testing for Different Classifiers\n",
    "When you are comparing 2 or more different machine learning classifiers in terms of their performance, several standard measures are used, e.g. accuracy, precision, recall and f1-score on a class-by-class basis, or for overall performance on all classes. What can strengthen your argument about showing the benefit of one classifier over the other is to show that the difference is *statistically significant*. \n",
    "\n",
    "Statistical significance testing in ML applications is often determining sufficient confidence in *rejecting the null hypothesis that is that there is no difference in the error distribution* of two models. Determining where there is sufficient confidence to reject the null hypothesis, this is usually determined by a p-value in a test being at or beneath a standard alpha threshold (e.g. p<=0.05 or p<=0.01 or p<=0.001).\n",
    "\n",
    "Beneath are some examples of some classification predictions from two different models, then an application of significance testing in terms of accuracy to test whether there is a statistically significant difference between the two models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19993b19",
   "metadata": {},
   "source": [
    "#  Example 1: from sklearn's NLP tutorial 'Working with Text Data' to determine whether one classifier is statistically significantly better than the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5e81ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45d56ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from machine_learning_utils.evaluation.significance_testing import calculate_mcnemar_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67670fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e03e63e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'soc.religion.christian',\n",
    "              'comp.graphics', 'sci.med']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6e0d819",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the 20 Newsgroups dataset\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bd4f442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with a random seed, always keep it the same number each time\n",
    "# for reproducibility (here 42 (=the meaning of life...))\n",
    "twenty_train = fetch_20newsgroups(subset='train',categories=categories, \n",
    "                                  shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40f8115b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2257"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fetch_20newsgroups puts the data in the .data attribute\n",
    "len(twenty_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80360339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting features from text data\n",
    "# Make sure you read the part of the tutorial/lecture about the bags of words\n",
    "# representation\n",
    "# A vectorizer is used to extract features from each item in the dataset\n",
    "\n",
    "\n",
    "# create a count vectorizer, which by default does some pre-processing\n",
    "# tokenize (into single words/unigrams) + lower-casing\n",
    "# to change these default settings look at the sklearn documentation\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f636b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First system: a Naive Bayes\n",
    "# Training a multinomial (beyond 2 class) NB classifier\n",
    "\n",
    "clf = MultinomialNB().fit(X_train_counts, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30bbcd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'God is love' => soc.religion.christian\n",
      "'OpenGL on the GPU is fast' => comp.graphics\n"
     ]
    }
   ],
   "source": [
    "# Testing on a toy dataset\n",
    "docs_new = ['God is love', 'OpenGL on the GPU is fast']\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "predicted = clf.predict(X_new_counts)\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, twenty_train.target_names[category]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c13ba6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Pipeline is an object that can carry out count extraction, weighting\n",
    "# and classification all in one go- be careful you know what each part does\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('clf', MultinomialNB()),\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1be50ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proper testing on the full 20newsgroups test set\n",
    "twenty_test = fetch_20newsgroups(subset='test', categories=categories, \n",
    "                                 shuffle=True, random_state=42)\n",
    "docs_test = twenty_test.data\n",
    "text_clf.fit(twenty_train.data, twenty_train.target)\n",
    "predicted_nb = text_clf.predict(docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8011ecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 0, 3, 0, 1, 3, 2, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first 10 predictions from the NB:\n",
    "predicted_nb[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4efad18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       0.92      0.90      0.91       319\n",
      "         comp.graphics       0.95      0.95      0.95       389\n",
      "               sci.med       0.96      0.91      0.93       396\n",
      "soc.religion.christian       0.91      0.97      0.94       398\n",
      "\n",
      "              accuracy                           0.93      1502\n",
      "             macro avg       0.93      0.93      0.93      1502\n",
      "          weighted avg       0.93      0.93      0.93      1502\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using the metrics package\n",
    "# Get a classification report to see overall and per-class performance \n",
    "print(metrics.classification_report(twenty_test.target, predicted_nb,\n",
    "                                    target_names=twenty_test.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c686415f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System 2: logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59c308f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf_2 = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('clf', (LogisticRegression())),\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "009017c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julianhough/miniforge3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "text_clf_2.fit(twenty_train.data, twenty_train.target)\n",
    "predicted_svm = text_clf_2.predict(docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2383687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       0.92      0.81      0.86       319\n",
      "         comp.graphics       0.86      0.94      0.90       389\n",
      "               sci.med       0.92      0.82      0.87       396\n",
      "soc.religion.christian       0.88      0.98      0.93       398\n",
      "\n",
      "              accuracy                           0.89      1502\n",
      "             macro avg       0.90      0.89      0.89      1502\n",
      "          weighted avg       0.89      0.89      0.89      1502\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(twenty_test.target, predicted_svm,\n",
    "                                    target_names=twenty_test.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6158d412",
   "metadata": {},
   "source": [
    "## Comparing the two sets of predictions against the ground truth and calculating significance using Macnemar's statistic\n",
    "While the Naive Bayes appears to outperform the Logistic Regression in terms of its overall accuracy/f-score being higher, is there a significantly different distributions in errors? For this we use Macnemar's statistic applied to Machine Learning, as per:\n",
    "\n",
    "    Dietterich, Thomas G. \"Approximate statistical tests for comparing supervised classification learning algorithms.\" Neural computation 10, no. 7 (1998): 1895-1923."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc816167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']\n",
      "statistic=51.000, p-value=0.000\n",
      "Different proportions of errors (reject H0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(51.0, 0.00025724063479780124, True, True, True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(twenty_test.target_names)\n",
    "calculate_mcnemar_test(twenty_test.target, predicted_svm, predicted_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5b1222",
   "metadata": {},
   "source": [
    "Overall, there is a statistically significant difference in error distributions with the null hypothesis that the error distributions being correct being rejected at p<=0.05, p<=0.01 and p<=0.001. Now let's check whether that's the case for every individual class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a112c7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism\n",
      "statistic=37.000, p-value=0.278\n",
      "Same proportions of errors (fail to reject H0)\n",
      "(37.0, 0.2779992890805517, False, False, False)\n",
      "******************************\n",
      "comp.graphics\n",
      "statistic=28.000, p-value=0.007\n",
      "Different proportions of errors (reject H0)\n",
      "(28.0, 0.007275502036308492, True, True, False)\n",
      "******************************\n",
      "sci.med\n",
      "statistic=27.000, p-value=0.000\n",
      "Different proportions of errors (reject H0)\n",
      "(27.0, 1.4766951457590834e-05, True, True, True)\n",
      "******************************\n",
      "soc.religion.christian\n",
      "statistic=26.000, p-value=0.207\n",
      "Same proportions of errors (fail to reject H0)\n",
      "(26.0, 0.20736789985685422, False, False, False)\n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "# check the significance for each class in a one-vs-rest fashion:\n",
    "for i, target in enumerate(twenty_test.target_names):\n",
    "    print(target)\n",
    "    result = calculate_mcnemar_test(twenty_test.target, predicted_svm, predicted_nb, target_class=i)\n",
    "    print(result)\n",
    "    print(\"*\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7778feb8",
   "metadata": {},
   "source": [
    "As can be seen, while overall there is a difference in performance between the two classifiers with the Naive Bayes outperforming the Logistic Regression overall, on the individual classes the performance is statistically insignificantly different for the alt.atheism and soc.religion.christian classes when considering p<=0.05 as signficant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daf5fec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
